# ğŸŒ¿ KCC Query Assistant â€” Offline AI with FAISS + LLaMA3

This project is a smart, offline-ready agricultural assistant designed to help farmers and agriculture officers get accurate crop-related answers using AI, even without an internet connection. It leverages semantic search using FAISS and sentence-transformers, and uses LLaMA3 via Ollama for LLM-based answer generation.

---

## âœ… Features

- ğŸ” Top-k semantic search using FAISS + MPNet sentence embeddings
- ğŸ§  Contextual answer generation with LLaMA3 through Ollama
- ğŸ§¾ Retrieval from official KCC (Kisan Call Center) agricultural dataset
- ğŸŒ Internet fallback with DuckDuckGo for general LLM response + search
- ğŸ’¡ Streamlit-based interactive user interface
- ğŸ‡®ğŸ‡³ Localized for Indian farming needs

---

## ğŸ’» Tech Stack

- `sentence-transformers/all-mpnet-base-v2`
- `FAISS` (CPU)
- `Ollama` with LLaMA3
- `Streamlit`
- `BeautifulSoup4` + `Requests` for fallback search

---

## ğŸš€ How to Run the Project

### Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 2: Launch the Assistant

```bash
streamlit run app/app.py
```

> Make sure Ollama is running and LLaMA3 is downloaded locally.

---

## ğŸ“ Project Structure

```
KCCQueryAssistant/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ 1_data_preprocessing.py
â”‚   â””â”€â”€ 2_generate_embeddings_and_faiss.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cleaned_kcc.jsonl
â”œâ”€â”€ vector_store/
â”‚   â”œâ”€â”€ texts.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Working Logic

1. User enters a question related to agriculture.
2. The system searches top-k relevant KCC entries using FAISS.
3. If context is found:
   - LLaMA3 generates the answer using the retrieved context.
4. If context is not found:
   - A fallback DuckDuckGo internet search is triggered.
   - The LLM still attempts a general response.

---

## ğŸŒ¾ Dataset Usage

- Official **KCC dataset of Karnataka** used.
- Original dataset size: **1.6 million entries**.
- Processed a representative **sample of 100,000 Q&A pairs** for embedding.

---

## ğŸŒ¾ Sample Use Case

**Input:**  
`Fertilizer for Tomato in Tumkur?`

**â†’**  
- Context retrieved from KCC dataset  
- Answer generated using LLaMA3  
- Fallback search shown if no local match exists

---

## ğŸ“Œ Notes

- LLaMA3 model must be available in Ollama locally
- Large files like `.faiss`, `.npy`, and `.csv` are excluded from the repo due to GitHubâ€™s 100MB limit
- The app is optimized for regional agricultural queries in India
- Can be extended with multilingual support and voice integration

---

## ğŸ“„ License

This submission is part of the ANNAM.AI academic assignment.  
All data is used for educational purposes only.

---
âœ… Developed for empowering Indian agriculture through AI
